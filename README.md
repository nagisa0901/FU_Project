## リカレントニューラルネットワーク（RNN）を用いた自動作曲システム
### 未来大3年次プロジェクト学習

・リカレントニューラルネットワークをフルスクラッチで実装<br>
・１つ音を入力されたら、次の音を予測する<br>
・入出力はMidiファイルで行う<br>

入力（出力）は<br>
"128のOnehotベクトル（Midiで表現できる音の範囲）"＋"音を出す（0）/出さない(1)"＋"11のOnehotベクトル（音符の種類（音の長さ））"
の計140のベクトルになっている<br>

sin波の予測は可能（左：Loss、右：予測結果）<br>
<img src="https://user-images.githubusercontent.com/38197391/50456552-9de1a300-0998-11e9-8366-7879fd31e003.png" width="500"><br>
K.C.Blues/Charlie Parkerを学習させた結果は以下の通り<br>
音の予測に関しては途中までは良いが、最終的に周期的になってしまう（右：x軸は時間、y軸は音の高さ）
<img src="https://user-images.githubusercontent.com/38197391/50456915-1fd2cb80-099b-11e9-8ef3-218c380b6b69.png" width="500">


### 結果
・音符の種類をうまく学習できなかったため、出力の際はすべて４分音符で出力した  
・音楽データは時系列データとしては複雑すぎる  
・RNNの発展形であるLSTMを使うといいのではないか  

